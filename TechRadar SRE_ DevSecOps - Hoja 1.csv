name,ring,quadrant,isNew,description;;;
"API expand-contract (swagger),Adopt,Techniques,TRUE,""<p>The <strong>API expand-contract</strong> pattern, sometimes called <a href=""""https://www.martinfowler.com/bliki/ParallelChange.html"""">parallel change</a>, will be familiar to many, especially when used with databases or code";" however, we only see low levels of adoption with APIs. Specifically, we're seeing complex versioning schemes and breaking changes used in scenarios where a simple expand and then contract would suffice. For example, first adding to an API while deprecating an existing element, and then only later removing the deprecated elements once consumers are switched to the newer schema. This approach does require some coordination and visibility of the API consumers, perhaps through a technique such as <a href=""""https://martinfowler.com/articles/consumerDrivenContracts.html"""">consumer-driven contract</a> testing.</p>""";;
"Continuous delivery for machine learning (CD4ML),Adopt,Techniques,FALSE,""<p>We see <strong><a href=""""https://martinfowler.com/articles/cd4ml.html"""">continuous delivery for machine learning (CD4ML)</a></strong> as a good default starting point for any ML solution that is being deployed into production. Many organizations are becoming more reliant on ML solutions for both customer offerings and internal operations so it makes sound business sense to apply the lessons and good practice captured by <a href=""""/radar/techniques/continuous-delivery-cd"""">continuous delivery (CD)</a> to ML solutions.</p>""";;;
"Service account rotation approach,Assess,Techniques,TRUE,""<p>We strongly advise organizations to make sure, when they really need to use cloud service accounts, that they are rotating the credentials. Rotation is one of <a href=""""/radar/techniques/the-three-rs-of-security"""">the three R's of security</a>. It is far too easy for organizations to forget about these accounts unless an incident occurs. This is leading to accounts with unnecessarily broad permissions remaining in use for long periods alongside a lack of planning for how to replace or rotate them. Regularly applying a cloud <strong>service account rotation approach</strong> also provides a chance to exercise the principle of least privilege.</p>""";;;
"Cloud sandboxes,Assess,Techniques,TRUE,""<p>As the cloud is becoming more and more a commodity and being able to spin up <strong>cloud sandboxes</strong> is easier and available at scale, our teams prefer cloud-only (as opposed to local) development environments to reduce maintenance complexity. We're seeing that the tooling to do local simulation of cloud-native services limits the confidence in developer build and test cycles";" therefore, we're looking to focus on standardizing cloud sandboxes over running cloud-native components on a developer machine. This will drive good <a href=""""/radar/techniques/infrastructure-as-code"""">infrastructure-as-code</a> practices as a forcing function and good onboarding processes for provisioning sandbox environments for developers. There are risks associated with this transition, as it assumes that developers will have an absolute dependency on cloud environment availability, and it may slow down the developer feedback loop. We strongly recommend you adopt some lean governance practices regarding standardization of these sandbox environments, especially with regard to security, IAM and regional deployments.</p>""";;
"GitOps,Trial,Techniques,TRUE,""<p>We suggest approaching <strong>GitOps</strong> with a degree of care, especially with regard to branching strategies. GitOps can be seen as a way of implementing <a href=""""/radar/techniques/infrastructure-as-code"""">infrastructure as code</a> that involves continuously synchronizing and applying infrastructure code from <a href=""""/radar/tools/git"""">Git</a> into various environments. When used with a """"branch per environment"""" infrastructure, changes are promoted from one environment to the next by merging code. While treating code as the single source of truth is clearly a sound approach, we're seeing branch per environment lead to environmental drift and eventually environment-specific configs as code merges become problematic or even stop entirely. This is very similar to what we've seen in the past with <a href=""""/radar/techniques/long-lived-branches-with-gitflow"""">long-lived branches with GitFlow</a>.</p>""";;;
"Peer Review of Production Changes,Trial,Techniques,TRUE,""<p>Some organizations seem to think <strong>peer review equals pull request</strong>";" they've taken the view that the only way to achieve a peer review of code is via a pull request. We've seen this approach create significant team bottlenecks as well as significantly degrade the quality of feedback as overloaded reviewers begin to simply reject requests. Although the argument could be made that this is one way to demonstrate code review """"regulatory compliance"""" one of our clients was told this was invalid since there was no evidence the code was actually read by anyone prior to acceptance. Pull requests are only one way to manage the code review workflow";" we urge people to consider other approaches, especially where there is a need to coach and pass on feedback carefully.</p>""";
Trunk based development,Adopt,Techniques,TRUE,;;;
Branch Per Feature Deployment,Assess,Techniques,TRUE,;;;
Service Levels (SLx),Adopt,Techniques,TRUE,;;;
Agentes Efimeros (VMs),Trial,Techniques,TRUE,;;;
IaC,Adopt,Techniques,TRUE,;;;
Version Control For All Production Artifacts,Trial,Techniques,TRUE,;;;
Automated Acceptance Testing,Trial,Techniques,TRUE,;;;
Foro Arq DevSecOps,Adopt,Techniques,TRUE,;;;
Continuous Testing,Trial,Techniques,TRUE,;;;
Immutable infrastructure,Assess,Techniques,TRUE,;;;
Dast / Sast scaning,Adopt,Techniques,TRUE,;;;
Security as a Value Stream,Trial,Techniques,TRUE,;;;
Compliance Monitoring,Assess,Techniques,TRUE,;;;
"AWS,Adopt,Platforms,FALSE,""<p>Many of our teams who are already on AWS have found <strong><a href=""""https://docs.aws.amazon.com/cdk/latest/guide/home.html"""">AWS Cloud Development Kit</a></strong> (AWS CDK) to be a sensible AWS default for enabling infrastructure provisioning. In particular, they like the use of first-class programming languages instead of configuration files which allows them to use existing tools, test approaches and skills. Like similar tools, care is still needed to ensure deployments remain easy to understand and maintain. The development kit currently supports <a href=""""/radar/languages-and-frameworks/typescript"""">TypeScript</a>, JavaScript, Python, Java, C# and .NET. New providers are being added to the CDK core. We've also used both AWS Cloud Development Kit and HashiCorp's <a href=""""https://learn.hashicorp.com/tutorials/terraform/cdktf"""">Cloud Development Kit for Terraform</a> to generate Terraform configurations and enable provisioning with the Terraform platform with success.</p>""";;;
"AZURE Cloud,Adopt,Platforms,FALSE,""<p>We continue to see interest in and use of <strong><a href=""""https://backstage.io/"""">Backstage</a></strong> grow, alongside the adoption of developer portals, as organizations look to support and streamline their development environments. As the number of tools and technologies increases, some form of standardization is becoming increasingly important for consistency so that developers are able to focus on innovation and product development instead of getting bogged down with reinventing the wheel. Backstage is an open-source developer portal platform created by Spotify, it's based upon software templates, unifying infrastructure tooling and consistent and centralized technical documentation. The plugin architecture allows for extensibility and adaptability into an organization’s infrastructure ecosystem.</p>""";;;
"GCP,Adopt,Platforms,FALSE,""<p><strong><a href=""""https://delta.io/"""">Delta Lake</a></strong> is an <a href=""""https://github.com/delta-io/delta"""">open-source storage layer</a>, implemented by Databricks, that attempts to bring ACID transactions to big data processing. In our Databricks-enabled <a href=""""/radar/techniques/data-lake"""">data lake</a> or <a href=""""/radar/techniques/data-mesh"""">data mesh</a> projects, our teams continue to prefer using Delta Lake storage over the direct use of file storage types such <a href=""""https://aws.amazon.com/s3/"""">S3</a> or <a href=""""https://azure.microsoft.com/en-au/services/storage/data-lake-storage/"""">ADLS</a>. Of course this is limited to projects that use storage platforms that support <a href=""""https://docs.delta.io/latest/delta-storage.html"""">Delta Lake</a> when using <a href=""""https://parquet.apache.org/"""">Parquet</a> file formats. Delta Lake facilitates concurrent data read/write use cases where file-level transactionality is required. We find Delta Lake's seamless integration with Apache Spark <a href=""""https://docs.databricks.com/delta/delta-batch.html"""">batch</a> and <a href=""""https://docs.databricks.com/delta/delta-streaming.html"""">micro-batch</a> APIs greatly helpful, particularly features such as <a href=""""https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html"""">time travel</a> — accessing data at a particular point in time or commit reversion — as well as <a href=""""https://databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html"""">schema evolution</a> support on write";" though there are some limitations on these features.</p>""";;
"Openshift,Adopt,Platforms,FALSE,""<p><strong><a href=""""https://materialize.io/"""">Materialize</a></strong> is a streaming database that enables you to do incremental computation without complicated data pipelines. Just describe your computations via standard SQL views and connect Materialize to the data stream. The underlying differential data flow engine performs incremental computation to provide consistent and correct output with minimal latency. Unlike traditional databases, there are no restrictions in defining these materialized views, and the computations are executed in real time. We've used Materialize, together with Spring Cloud Stream and Kafka, to query over streams of events for insights in a distributed event-driven system, and we quite like the setup.</p>""";;;
"Kubernetes,Adopt,Platforms,FALSE,""<p>Since we last mentioned <strong><a href=""""https://www.snowflake.com/"""">Snowflake</a></strong> in the Radar, we've gained more experience with it as well as with <a href=""""/radar/techniques/data-mesh"""">data mesh</a> as an alternative to data warehouses and lakes. Snowflake continues to impress with features like time travel, zero-copy cloning, data sharing and its marketplace. We also haven't found anything we don't like about it, all of which has led to our consultants generally preferring it over the alternatives. Redshift is moving toward storage and compute separation, which has been a strong point of Snowflake, but even with Redshift Spectrum it isn't as convenient and flexible to use, partly because it is bound by its Postgres heritage (we do still like <a href=""""/radar/platforms/postgresql-for-nosql"""">Postgres</a>, by the way). Federated queries can be a reason to go with Redshift. When it comes to operations, Snowflake is much simpler to run. <a href=""""/radar/platforms/bigquery"""">BigQuery</a>, which is another alternative, is very easy to operate, but in a multicloud setup Snowflake is a better choice. We can also report that we've used Snowflake successfully with <a href=""""/radar/platforms/google-cloud-platform"""">GCP</a>, <a href=""""/radar/platforms/aws"""">AWS</a>, and <a href=""""/radar/platforms/azure"""">Azure</a>.</p>""";;;
"Docker/ Container Instances,Trial,Platforms,TRUE,""<p><strong>Variable fonts</strong> are a way of avoiding the need to find and include separate font files for different weights and styles. Everything is in one font file, and you can use properties to select which style and weight you need. While not new, we still see sites and projects that could benefit from this simple approach. If you have pages that are including many variations of the same font, we suggest trying out variable fonts.</p>""";;;
"Functions / Lambda,Trial,Platforms,TRUE,""<p><strong><a href=""""https://pinot.apache.org/"""">Apache Pinot</a></strong> is a distributed OLAP data store, built to deliver real-time analytics with low latency. It can ingest from batch data sources (such as Hadoop HDFS, Amazon S3, Azure ADLS or Google Cloud Storage) as well as stream data sources (such as Apache Kafka). If the need is user-facing, low-latency analytics, SQL-on-Hadoop solutions don't offer the low latency that is needed. Modern OLAP engines like Apache Pinot (or <a href=""""https://druid.apache.org/"""">Apache Druid</a> and <a href=""""https://clickhouse.tech/"""">Clickhouse</a> among others) can achieve much lower latency and are particularly suited in contexts where fast analytics, such as aggregations, are needed on immutable data, possibly, with real-time data ingestion. Originally built by LinkedIn, Apache Pinot entered Apache incubation in late 2018 and has since added a plugin architecture and SQL support among other key capabilities. Apache Pinot can be fairly complex to operate and has many moving parts, but if your data volumes are large enough and you need low-latency query capability, we recommend you assess Apache Pinot.</p>""";;;
"PaaS,Trial,Platforms,TRUE,""<p><strong><a href=""""https://bit.dev/"""">Bit.dev</a></strong> is a cloud-hosted collaborative platform for UI components extracted, modularized and reused with <a href=""""https://github.com/teambit/bit"""">Bit</a>. <a href=""""/radar/platforms/web-components-standard"""">Web components</a> have been around for a while, but building a modern front-end application by assembling small, independent components extracted from other projects has never been easy. Bit was designed to let you do exactly that: extract a component from an existing library or project. You can either build your own service on top of Bit for component collaboration or use Bit.dev.</p>""";;;
"SaaS,Adopt,Platforms,TRUE,""<p>Since we first mentioned <a href=""""/radar/techniques/data-discoverability"""">data discoverability</a> in the Radar, LinkedIn has evolved <a href=""""https://engineering.linkedin.com/blog/2016/03/open-sourcing-wherehows--a-data-discovery-and-lineage-portal"""">WhereHows</a> to <strong><a href=""""https://github.com/linkedin/datahub"""">DataHub</a></strong>, the next generation platform that addresses data discoverability via an extensible metadata system. Instead of crawling and pulling metadata, DataHub adopts a push-based model where individual components of the data ecosystem publish metadata via an API or a stream to the central platform. This push-based integration shifts the ownership from the central entity to individual teams making them accountable for their metadata. As more and more companies are trying to become data driven, having a system that helps with data discovery and understanding data quality and lineage is critical, and we recommend you assess DataHub in that capacity.</p>""";;;
"IaaS,Assess,Platforms,TRUE,""<p><strong><a href=""""https://www.featurestore.org/"""">Feature Store</a></strong> is an ML-specific data platform that addresses some of the key challenges we face today in feature engineering with three fundamental capabilities: (1) it uses managed data pipelines to remove struggles with pipelines as new data arrives"; (2) catalogs and stores feature data to promote discoverability and collaboration of features across models; and (3) consistently serves feature data during training and interference. </p>;
"<p>Since Uber revealed their <a href=""""https://eng.uber.com/michelangelo-machine-learning-platform/"""">Michelangelo platform</a>, many organizations and startups have built their own versions of a feature store";" examples include <a href=""""https://github.com/logicalclocks/hopsworks"""">Hopsworks</a>, <a href=""""https://github.com/feast-dev/feast"""">Feast</a> and <a href=""""https://www.tecton.ai/"""">Tecton</a>. We see potential in Feature Store and recommend you carefully assess it.</p>""";;
"ElasticCloud / Search,Adopt,Platforms,TRUE,""<p><strong><a href=""""https://github.com/juicedata/juicefs"""">JuiceFS</a></strong> is an open-source, distributed POSIX file system built on top of <a href=""""https://www.thoughtworks.com/radar/platforms/redis"""">Redis</a> and an object store service (for example, Amazon S3). If you're building new applications, then our recommendation has always been to interact directly with the object store without going through another abstraction layer. However, JuiceFS can be an option if you're migrating legacy applications that depend on traditional POSIX file systems to the cloud.</p>""";;;
"Kibana,Adopt,Platforms,TRUE,""<p>As more businesses turn to events as a way to share data among microservices, collect analytics or feed data lakes, <a href=""""/radar/tools/apache-kafka"""">Apache Kafka</a> has become a favorite platform to support an event-driven architectural style. Although Kafka was a revolutionary concept in scalable persistent messaging, a lot of moving parts are required to make it work, including ZooKeeper, brokers, partitions, and mirrors. While these can be particularly tricky to implement and operate, they do offer great flexibility and power when needed, especially at an industrial enterprise scale. Because of the high barrier to entry presented by the full Kafka ecosystem, we welcome the recent explosion of platforms offering the <strong>Kafka API without Kafka</strong>. Recent entries such as <a href=""""https://github.com/streamnative/kop"""">Kafka on Pulsar</a> and <a href=""""/radar/platforms/redpanda"""">Redpanda</a> offer alternative architectures, and <a href=""""https://github.com/Azure/azure-event-hubs-for-kafka"""">Azure Event Hubs for Kafka</a> provides some compatibility with Kafka producer and consumer APIs. Some features of Kafka, like the streams client library, are not compatible with these alternative brokers, so there are still reasons to choose Kafka over alternative brokers. It remains to be seen, however, if developers actually adopt this strategy or if it is merely an attempt by competitors to lure users away from the Kafka platform. Ultimately, perhaps Kafka's most enduring impact could be the convenient protocol and API provided to clients.</p>""";;;
"Grafana,Trial,Platforms,TRUE,""<p><a href=""""https://nats.io/about/""""><strong>NATS</strong></a> is a fast, secure message queueing system with an unusually wide range of features and potential deployment targets. At first glance, you would be forgiven for asking why the world needs another message queueing system. Message queues have been around in various forms for nearly as long as businesses have been using computers and have undergone years of refinement and optimization for various tasks. But NATS has several interesting characteristics and is unique in its ability to scale from embedded controllers to global, cloud-hosted superclusters. We're particularly intrigued by NATS's intent to support a continuous streaming flow of data from mobile devices and IoT and through a network of interconnected systems. However, some tricky issues need to be addressed, not the least of which is ensuring consumers see only the messages and topics to which they're allowed access, especially when the network spans organizational boundaries. NATS 2.0 introduced a security and access control framework that supports multitenant clusters where accounts restrict a user's access to queues and topics. Written in Go, NATS has primarily been embraced by the Go language community. Although clients exist for pretty much all widely used programming languages, the Go client is by far the most popular. However, some of our developers have found that all the language client libraries tend to reflect the Go origins of codebase. Increasing bandwidth and processing power on small, wireless devices means that the volume of data businesses must consume in real time will only increase. Assess NATS as a possible platform for streaming that data within and among businesses.</p>""";;;
"Dynatrace,Trial,Platforms,TRUE,""<p><strong><a href=""""https://opstrace.com/"""">Opstrace</a></strong> is an open-source observability platform intended to be deployed in the user's own network. If we don't use commercial solutions like Datadog (for example, because of cost or data residency concerns), the only solution is to build your own platform composed of open-source tools. This can take a lot of effort — Opstrace is intended to fill this gap. It uses open-source APIs and interfaces such as <a href=""""/radar/tools/prometheus"""">Prometheus</a> and <a href=""""/radar/tools/grafana"""">Grafana</a> and adds additional features on top like TLS and authentication. At the heart of Opstrace runs a <a href=""""https://github.com/cortexproject/cortex"""">Cortex</a> cluster to provide the scalable Prometheus API as well as a <a href=""""https://github.com/grafana/loki"""">Loki</a> cluster for the logs. It's fairly new and <a href=""""https://opstrace.com/docs/references/roadmap#opstrace-roadmap"""">still misses features</a> when compared to solutions like Datadog or SignalFX. Still, it's a promising addition to this space and worth keeping an eye on.</p>""";;;
"AppsD,Trial,Platforms,FALSE,""<p>We've seen interest in <strong><a href=""""https://pulumi.io/"""">Pulumi</a></strong> slowly but steadily rising. Pulumi fills a gaping hole in the infrastructure coding world where <a href=""""/radar/tools/terraform"""">Terraform</a> maintains a firm hold. While Terraform is a tried-and-true standby, its declarative nature suffers from inadequate abstraction facilities and limited testability. Terraform is adequate when the infrastructure is entirely static, but dynamic infrastructure definitions call for a real programming language. Pulumi distinguishes itself by allowing configurations to be written in <a href=""""/radar/languages-and-frameworks/typescript"""">TypeScript</a>/JavaScript, <a href=""""/radar/languages-and-frameworks/python-3"""">Python</a> and <a href=""""/radar/languages-and-frameworks/go-language"""">Go</a> — no markup language or templating required. Pulumi is tightly focused on cloud-native architectures — including containers, serverless functions and data services — and provides good support for <a href=""""/radar/platforms/kubernetes"""">Kubernetes</a>. Recently, <a href=""""/radar/platforms/aws-cloud-development-kit"""">AWS CDK</a> has mounted a challenge, but Pulumi remains the only cloud-neutral tool in this area. We're anticipating wider Pulumi adoption in the future and looking forward to a viable tool and knowledge ecosystem emerging to support it.</p>""";;;
"OpenTelemetry,Adopt,Platforms,TRUE,""<p><strong><a href=""""https://github.com/vectorizedio/redpanda"""">Redpanda</a></strong> is a streaming platform that provides a <a href=""""/radar/tools/apache-kafka"""">Kafka</a>-compatible API, allowing it to benefit from the Kafka ecosystem without having to deal with the complexities of a Kafka installation. For example, Redpanda simplifies operations by shipping as a single binary and avoiding the need for an external dependency such as ZooKeeper. Instead, it implements the Raft protocol and performs comprehensive tests to validate it's been <a href=""""https://vectorized.io/blog/validating-consistency/"""">implemented correctly</a>. One of Redpanda’s capabilities (available for enterprise customers only) is inline <a href=""""/radar/languages-and-frameworks/webassembly"""">WebAssembly (WASM)</a> transformations, using an embedded WASM engine. This allows developers to create event transformers in their language of choice and compile it to WASM. Redpanda also offers much reduced tail latencies and increased throughput due to a <a href=""""https://vectorized.io/blog/tpc-buffers/"""">series of optimizations</a>. Redpanda is an exciting alternative to Kafka and is worth assessing.</p>""";;;
"onPrem,Hold,Platforms,TRUE,""<p>We've observed before that the cloud providers push more and more services onto the market. We've also documented our concerns that sometimes the services are made available when they're not quite ready for prime time. Unfortunately, in our experience, <strong><a href=""""https://azure.microsoft.com/en-us/services/machine-learning/"""">Azure Machine Learning</a></strong> falls into the latter category. One of several <a href=""""https://towardsdatascience.com/top-8-no-code-machine-learning-platforms-you-should-use-in-2020-1d1801300dd0"""">recent entrants</a> in the field of <a href=""""/radar/techniques/bounded-low-code-platforms"""">bounded low-code platforms</a>, Azure ML promises more convenience for data scientists. Ultimately, however, it doesn't live up to its promise";" in fact, it still feels easier for our data scientists to work in Python. Despite significant efforts, we struggled to make it scale and lack of adequate documentation proved to be another issue which is why we moved it to the Hold ring.</p>""";;
"Mesh,Trial,Platforms,TRUE,""<p>Products supported by companies or communities are in constant evolution, at least the ones that get traction in the industry. Sometimes organizations tend to build frameworks or abstractions on top of the existing external products to cover very specific needs, thinking that the adaptation will provide more benefits than the existing ones. We're seeing organizations trying to create <strong>homemade infrastructure-as-code (IaC) products</strong> on top of the existing ones"; they underestimate the required effort to keep those solutions evolving according to their needs, and after a short period of time, they realize that the original version is in much better shape than their own;" there are even cases where the abstraction on top of the external product reduces the original capabilities. Although we've seen success stories of organizations building homemade solutions, we want to caution about this approach as the effort required to do so isn't negligible, and a long-term product vision is required to have the expected outcomes.</p>""";
"Azure Devops,Adopt,Tools,FALSE,""<p><strong><a href=""""https://sentry.io/"""">Sentry</a></strong> has become the default choice for many of our teams when it comes to front-end error reporting. The convenience of features like the grouping of errors or defining patterns for discarding errors with certain parameters helps deal with the flood of errors coming from many end user devices. Integrating Sentry in your CD pipeline allows you to upload source maps for more efficient error debugging, and it helps easily trace back which errors occurred in which version of the software. We also appreciate that while Sentry is primarily a SaaS offering, its source code is publicly available and it can be used for free for smaller use cases and <a href=""""https://develop.sentry.dev/self-hosted/"""">self-hosting</a>.</p>""";;;
"Azure Repos,Adopt,Tools,TRUE,""<p>Making the web inclusive requires serious attention to ensure accessibility is considered <em>and</em> validated at all stages of software delivery. Many of the popular accessibility testing tools are designed for testing after a web application is complete";" as a result, issues are detected late and often are harder to fix, accumulating as debt. In our recent internal work on ThoughtWorks websites, we included the open-source accessibility (a11y) testing engine <strong><a href=""""https://github.com/dequelabs/axe-core"""">axe-core</a></strong> as part of our build processes. It provided team members with early feedback on adherence to accessibility rules, even during early increments. Not every issue can be found through automated inspection, though. Extending the functionality of axe-core is the commercially available <a href=""""https://www.deque.com/axe/devtools/"""">axe DevTools</a>, including functionality that guides team members through exploratory testing for a majority of accessibility issues.</p>""";;
"SonarCloud,Adopt,Tools,FALSE,""<p>Since we last wrote about <strong><a href=""""https://www.getdbt.com/"""">dbt</a></strong>, we've used it in a few projects and like what we've seen. For example, we like that dbt makes the transformation part of ELT pipelines more accessible to consumers of the data as opposed to just the data engineers building the pipelines. It does this while encouraging good engineering practices such as versioning, automated testing and deployment. SQL continues to be the lingua franca of the data world (including databases, warehouses, query engines, data lakes and analytical platforms) and most of these systems support it to some extent. This allows dbt to be used against these systems for transformations by just building adaptors. The number of native connectors has grown to include <a href=""""/radar/platforms/snowflake"""">Snowflake</a>, <a href=""""/radar/platforms/bigquery"""">BigQuery</a>, Redshift and Postgres, as has the range of <a href=""""https://docs.getdbt.com/docs/available-adapters"""">community plugins</a>. We see tools like dbt helping data platforms become more """"self service"""" capable.</p>""";;;
"GitHub,Hold,Tools,TRUE,""<p>We've always been keen to find tools that can shorten the software development feedback cycle";" <strong><a href=""""https://github.com/evanw/esbuild"""">esbuild</a></strong> is such an example. As the front-end codebase grows larger, we usually face a packaging time of minutes. As a JavaScript bundler optimized for speed, esbuild can reduce this time by a factor of 10 to 100. It is written in Golang and uses a more efficient approach in the process of parsing, printing and source map generation which significantly surpasses build tools such as <a href=""""/radar/tools/webpack"""">Webpack</a> and <a href=""""/radar/tools/parcel"""">Parcel</a> in building time. esbuild may not be as comprehensive as those tools in JavaScript syntax transformation";" however, this doesn't stop many of our teams from switching to esbuild as their default.</p>""";
"BitRise,Adopt,Tools,TRUE,""<p><strong><a href=""""https://github.com/facebook/flipper"""">Flipper</a></strong> is an extensible mobile application debugger. Out of the box it supports profiling, interactive layout inspection, log viewer and a network inspector for iOS, Android and <a href=""""/radar/languages-and-frameworks/react-native"""">React Native</a> applications. Compared to other debugging tools for mobile apps, we find Flipper to be lightweight, feature rich and easy to set up.</p>""";;;
"Jenkins,Hold,Tools,FALSE,""<p>We wrote about <strong><a href=""""https://docs.greatexpectations.io/en/latest/"""">Great Expectations</a></strong> in the previous edition of the Radar. We continue to like it and have moved it to Trial in this edition. Great Expectations is a framework that enables you to craft built-in controls that flag anomalies or quality issues in data pipelines. Just as unit tests run in a build pipeline, Great Expectations makes assertions during execution of a data pipeline. We like its simplicity and ease of use — the rules stored in JSON can be modified by our data domain experts without necessarily needing data engineering skills.</p>""";;;
"BitBucket,Hold,Tools,FALSE,""<p>We've had a bit more experience performance testing with <a href=""""https://k6.io/""""><strong>k6</strong></a> since we first covered it in the Radar, and with good results. Our teams have enjoyed the focus on the developer experience and flexibility of the tool. Although it's easy to get started with k6 all on its own, it really shines with its ease of integration into a developer ecosystem. For example, using the <a href=""""https://k6.io/docs/results-visualization/datadog"""">Datadog adapter</a>, one team was quickly able to visualize performance in a distributed system and identify significant concerns before releasing the system to production. Another team, with the commercial version of k6, was able to use the <a href=""""https://marketplace.visualstudio.com/items?itemName=k6.k6-load-test&ssr=false"""">Azure pipelines marketplace extension</a> to wire performance tests into their CD pipeline and get Azure DevOps reporting with little effort. Since k6 supports thresholds that allow for automated testing assertions out of the box, it's relatively easy to add a stage to your pipeline that detects performance degradation of new changes, adding a powerful feedback mechanism for developers.</p>""";;;
"AWS CodePipeline,Trial,Tools,FALSE,""<p><strong><a href=""""https://mlflow.org/"""">MLflow</a></strong> is an open-source tool for <a href=""""/radar/tools/experiment-tracking-tools-for-machine-learning"""">machine-learning experiment tracking</a> and lifecycle management. The workflow to develop and continuously evolve a machine-learning model includes a series of experiments (a collection of runs), tracking the performance of these experiments (a collection of metrics) and tracking and tweaking models (projects). MLflow facilitates this workflow nicely by supporting existing open standards and integrates well with many other tools in the ecosystem. <a href=""""https://databricks.com/product/managed-mlflow"""">MLflow as a managed service by Databricks</a> on the cloud, available in <a href=""""/radar/platforms/aws"""">AWS</a> and <a href=""""/radar/platforms/azure"""">Azure</a>, is rapidly maturing, and we've used it successfully in our projects. We find MLflow a great tool for model management and tracking, supporting both UI-based and API-based interaction models. Our only growing concern is that MLflow is attempting to deliver too many conflating concerns as a single platform, such as model serving and scoring.</p>""";;;
"Jira,Adopt,Tools,TRUE,""<p><strong><a href=""""https://developers.google.com/optimization"""">OR-Tools</a></strong> is an open-source software suite for solving combinatorial optimization problems. These optimization problems have a very large set of possible solutions, and tools like OR-Tools are quite helpful in seeking the best solution. You can model the problem in any one of the supported languages — Python, Java, C# or C++ — and choose the solvers from several supported open-source or commercial solvers. We've successfully used OR-Tools in multiple optimization projects with integer and mixed-integer programming.</p>""";;;
"Slack,Adopt,Tools,FALSE,""<p><strong><a href=""""https://playwright.dev/"""">Playwright</a></strong> allows you to write Web UI tests for Chromium and Firefox as well as WebKit, all through the same API. The tool has gained some attention for its support of all the major browser engines which it achieves by including patched versions of Firefox and Webkit. We continue to hear positive experience reports with Playwright, in particular its stability. Teams have also found it easy to migrate from <a href=""""/radar/languages-and-frameworks/puppeteer"""">Puppeteer</a>, which has a very similar API.</p>""";;;
"Teams,Trial,Tools,TRUE,""<p>We welcome the increased availability and maturity of <a href=""""/radar/techniques/infrastructure-configuration-scanner"""">infrastructure configuration scanning</a> tools: <strong><a href=""""https://github.com/toniblyx/prowler"""">Prowler</a></strong> helps teams scan their AWS infrastructure setups and improve security based on the results. Although Prowler has been around for a while, it has evolved a lot over the past few years, and we've found it very valuable to enable teams to take responsibility for proper security with a short feedback loop. Prowler categorizes <a href=""""https://d0.awsstatic.com/whitepapers/compliance/AWS_CIS_Foundations_Benchmark.pdf"""">AWS CIS benchmarking</a> checks into different groups (Identity and Access Management, Logging, Monitoring, Networking, CIS Level 1, CIS Level 2, EKS-CIS), and it includes many checks that help you gain insights into your PCI DSS and GDPR compliance.</p>""";;;
"Trello,Hold,Tools,TRUE,""<p>While <a href=""""https://en.wikipedia.org/wiki/Duck_typing"""">duck typing</a> is certainly seen as a feature by many Python programmers, sometimes — especially for larger codebases — type checking can be useful, too. For that reason a number of type annotations are proposed as Python Enhancement Proposals (PEPs), and <strong><a href=""""https://github.com/Microsoft/pyright"""">Pyright</a></strong> is a type checker that works with these annotations. In addition, it provides some type inference and guards that understand conditional code flow constructs. Designed with large codebases in mind, Pyright is fast, and its watch mode checks happen incrementally as files are changed to further shorten the feedback cycle. Pyright can be used directly on the command line, but integrations for VS Code, Emacs, vim, Sublime, and possibly other editors are available, too. In our experience, Pyright is preferable to alternatives like mypy.</p>""";;;
"ArgoCD (GitOps),Trial,Tools,TRUE,""<p>Adopting a """"you build it, you run it"""" DevOps philosophy means teams have increased attention on both technical and business metrics that can be extracted from the systems they deploy. Often we find that analytics tooling is difficult to access for most developers, so the work to capture and present metrics is left to other teams — long after features are shipped to end users. Our teams have found <strong><a href=""""https://redash.io/"""">Redash</a></strong> to be very useful for querying product metrics and creating dashboards in a way that can be self-served by general developers, shortening feedback cycles and focusing the whole team on the business outcomes.</p>""";;;
"CircleCI,Hold,Tools,FALSE,""<p><strong><a href=""""https://github.com/gruntwork-io/terratest"""">Terratest</a></strong> caught our attention in the past as an interesting option for infrastructure testing. Since then, our teams have been using it, and they're very excited about it because of its stability and the experience it provides. Terratest is a Golang library that makes it easier to write automated tests for infrastructure code. Using infrastructure-as-code tools such as <a href=""""/radar/tools/terraform"""">Terraform</a>, you can create real infrastructure components (such as servers, firewalls, or load balancers) to deploy applications on them and then validate the expected behavior using Terratest. At the end of the test, Terratest can undeploy the apps and clean up resources. This makes it largely useful for end-to-end tests of your infrastructure in a real environment.</p>""";;;
"Locus,Trial,Tools,TRUE,""<p><strong><a href=""""https://tuple.app/"""">Tuple</a></strong> is a relatively new tool optimized for remote paired programming, designed to fill the gap Slack left in the marketplace after abandoning Screenhero. Although it still exhibits some growing pains — platform availability is limited to Mac OS for now (with Linux support coming soon), and it has some UI quirks to work through — we've had good experience using it within those constraints. Unlike general-purpose video- and screen-sharing tools like Zoom, Tuple supports dual control with two mouse cursors, and unlike options such as <a href=""""/radar/tools/visual-studio-live-share"""">Visual Studio Live Share</a>, it isn't tied to an IDE. Tuple supports voice and video calls, clipboard sharing, and lower latency than general-purpose tools";" and its ability to let you draw and erase in your pair's screen with ease makes Tuple a very intuitive and developer-friendly tool.</p>""";;
"Postman,Trial,Tools,TRUE,""<p>When working with <a href=""""/radar/languages-and-frameworks/react-js"""">React</a>, we often encounter situations where our page is very slow because some components are re-rendering when they shouldn't be. <strong><a href=""""https://github.com/welldone-software/why-did-you-render"""">Why Did You Render</a></strong> is a library that helps detect why a component is re-rendering. It does this by monkey patching React. We've used it in a few of our projects to debug performance issues with great effect.</p>""";;;
"jmetter,Trial,Tools,TRUE,""<p>Even though <a href=""""/radar/platforms/docker"""">Docker</a> has become the sensible default for containerization, we're seeing new players in this space that are catching our attention. That is the case for <strong><a href=""""https://github.com/containers/buildah"""">Buildah</a> and <a href=""""https://github.com/containers/podman"""">Podman</a></strong>, which are complementary projects to build images (Buildah) and run containers (Podman) using a <a href=""""/radar/platforms/rootless-containers"""">rootless</a> approach in multiple Linux distributions. Podman introduces a daemonless engine for managing and running containers which is an interesting approach in comparison to what Docker does. The fact that Podman can use either <a href=""""https://opencontainers.org/"""">Open Container Initiative (OCI)</a> images built by Buildah or Docker images makes this tool even more attractive and easy to use.</p>""";;;
"Ansible,Adopt,Tools,TRUE,""<p>CI servers and build tools are some of the oldest and most widely used in our kit. They run the gamut from simple cloud-hosted services to complex, code-defined pipeline servers that support fleets of build machines. Given our experience and the wide range of options already available, we were initially skeptical when <strong><a href=""""https://docs.github.com/en/actions"""">GitHub Actions</a></strong> were introduced as another mechanism to manage the build and integration workflow. But the opportunity for developers to start small and easily customize behavior means that GitHub Actions are moving toward the default category for smaller projects. It's hard to argue with the convenience of having the build tool integrated directly into the source code repository. An enthusiastic community has emerged around this feature and that means a wide range of user-contributed tools and workflows are available to get started. Tools vendors are also getting on board via the <a href=""""https://github.com/marketplace?type=actions"""">GitHub Marketplace</a>. However, we still recommend you proceed with caution. Although code and <a href=""""/radar/tools/git"""">Git</a> history can be exported into alternative hosts, a development workflow based on GitHub Actions can't. Also, use your best judgment to determine when a project is large or complex enough to warrant an independently supported pipeline tool. But for getting up and running quickly on smaller projects, it's worth considering GitHub Actions and the ecosystem that is growing around them.</p>""";;;
"Terraform,Adopt,Tools,TRUE,""<p><strong><a href=""""https://www.graalvm.org/reference-manual/native-image/"""">Graal Native Image</a></strong> is a technology that compiles Java code into an operating system's native binary — in the form of a statically linked executable or a shared library. A native image is optimized to reduce the memory footprint and startup time of an application. Our teams have successfully used Graal native images, executed as small Docker containers, in the <a href=""""/radar/techniques/serverless-architecture"""">serverless architecture</a> where reducing start time matters. Although designed for use with programming languages such as <a href=""""/radar/languages-and-frameworks/go-language"""">Go</a> or <a href=""""/radar/languages-and-frameworks/rust"""">Rust</a> that natively compile and require smaller binary sizes and shorter start times, Graal Native Image can be equally useful to teams that have other requirements and want to use JVM-based languages.</p>";;;
"<p>Graal Native Image Builder, <em>native-image</em>, supports JVM-based languages — such as Java, Scala, Clojure and Kotlin — and builds executables on multiple operating systems including Mac OS, Windows and multiple distributions of Linux. Since it requires a closed-world assumption, where all code is known at compile time, additional configuration is needed for features such as <em>reflection</em> or <em>dynamic class loading</em> where types can't be deduced at build time from the code alone.</p>""";;;
"Prisma,Trial,Tools,TRUE,""<p><strong><a href=""""https://www.boundaryproject.io/"""">HashiCorp Boundary</a></strong> combines the secure networking and identity management capabilities needed for brokering access to your hosts and services in one place and across a mix of cloud and on-premise resources if needed. Key management can be done by integrating the key management service of your choice, be it from a cloud vendor or something like <a href=""""/radar/tools/hashicorp-vault"""">HashiCorp Vault</a>. HashiCorp Boundary supports a growing number of identity providers and can be integrated with parts of your service landscape to help define permissions, not just on host but also on a service level. For example, it enables you to control fine-grained access to a Kubernetes cluster, and dynamically pulling in service catalogs from various sources is on the roadmap. All of this stays out of the way of the engineering end users who get the shell experience they're used to, securely connected through Boundary's network management layer.</p>""";;;
"Rapid7,Trial,Tools,TRUE,""<p>Remember the research project <a href=""""https://github.com/tonybeltramelli/pix2code"""">pix2code</a> that showed how to automatically generate code from GUI screenshots? Now there is a productized version of this technique — <strong><a href=""""https://www.imgcook.com/"""">imgcook</a></strong> is a SaaS product from Alibaba that can intelligently transform various design files (Sketch/PSD/static images) into front-end code. Alibaba needs to customize a large number of campaign pages during the Double Eleven shopping festival. These are usually one-time pages that need to be developed quickly. Through the deep-learning method, the UX's design is initially processed into front-end code and then adjusted by the developer. Our team is evaluating this tech: although the image processing takes place on the server side while the main interface is on the web, imgcook provides <a href=""""https://github.com/imgcook"""">tools</a> that could integrate with the software design and development lifecycle. imgcook can generate static code as well as some data-binding component code if you define a DSL. The technology is not perfect yet";" designers need to refer to certain specifications to improve the accuracy of code generation (which still needs to be adjusted by developers afterward). We've always been cautious about magic code generation, because the generated code is usually difficult to maintain in the long run, and imgcook is no exception. But if you limit the usage to a specific context, such as one-time campaign pages, it's worth a try.</p>""";;
"Fortify,Adopt,Tools,TRUE,""<p><strong><a href=""""https://longhorn.io/"""">Longhorn</a></strong> is a distributed block storage system for <a href=""""/radar/platforms/kubernetes"""">Kubernetes</a>. There are many <a href=""""https://kubernetes-csi.github.io/docs/drivers.html"""">persistent storage options</a> for Kubernetes";" unlike most, however, Longhorn is built from the ground up to provide incremental snapshots and backups, thereby easing the pain of running a replicated storage for non–cloud-hosted Kubernetes. With the recent <a href=""""https://longhorn.io/blog/longhorn-v1.1.0/"""">experimental support for ReadWriteMany (RWX)</a> you can even mount the same volume for read and write access across many nodes. Choosing the right storage system for Kubernetes is a nontrivial task, and we recommend you assess Longhorn based on your needs.</p>""";;
"jUnit,Assess,Tools,TRUE,""<p><strong><a href=""""https://operatorframework.io/"""">Operator Framework</a></strong> is a set of open-source tools that simplifies building and managing the lifecycle of <a href=""""https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"""">Kubernetes operators</a>. The Kubernetes operator pattern, originally <a href=""""https://web.archive.org/web/20170129131616/https://coreos.com/blog/introducing-operators.html"""">introduced by CoreOS</a>, is an approach to encapsulate the knowledge of operating an application using Kubernetes native capabilities";" it includes <em>resources</em> to be managed and <em>controller code</em> that ensures the resources are matching their target state. This approach has been used to extend Kubernetes to manage <a href=""""https://operatorhub.io/"""">many applications</a>, particularly the stateful ones, natively. Operator Framework has three components: <a href=""""https://sdk.operatorframework.io/"""">Operator SDK</a>, which simplifies building, testing and packaging Kubernetes operators";" <a href=""""https://github.com/operator-framework/operator-lifecycle-manager/"""">Operator lifecycle manager</a> to install, manage and upgrade the operators";" and a <a href=""""https://operatorhub.io/"""">catalog</a> to publish and share third-party operators. Our teams have found Operator SDK particularly powerful in rapidly developing Kubernetes-native applications.</p>"""
"Selenium,Adopt,Tools,TRUE,""<p>The number of services offered by the big cloud providers keeps growing, but so does the convenience and maturity of tools that help you use them securely and efficiently. <a href=""""https://cloud.google.com/recommender""""><strong>Recommender</strong></a> is a service on Google Cloud that analyzes your resources and gives you recommendations on how to optimize them based on your actual usage. The service consists of a range of """"recommenders"""" in areas such as security, compute usage or cost savings. For example, the <a href=""""https://cloud.google.com/iam/docs/role-recommendations"""">IAM Recommender</a> helps you better implement the principle of least privilege by pointing out permissions that are never actually used and therefore are potentially too broad.</p>""";;;
"Cypress,Adopt,Tools,TRUE,""<p>Over the past few years <a href=""""https://docs.microsoft.com/en-us/windows/wsl/"""">Windows Subsystem for Linux (WSL)</a> has come up a few times in our discussions. Although we liked what we saw, including the improvements in WSL 2, it never made it into the Radar. In this edition we want to highlight an extension for Visual Studio Code that greatly improves the experience working with WSL. Although Windows-based editors could always access files on a WSL file system, they were unaware of the isolated Linux environment. With the <strong><a href=""""https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-wsl"""">Remote - WSL</a></strong> extension, Visual Studio Code becomes aware of WSL, allowing developers to launch a Linux shell. This also enables debugging of binaries running inside WSL from Windows. Jetbrains' IntelliJ too has seen steady improvement in its <a href=""""https://youtrack.jetbrains.com/issue/IDEA-171510#focus=Comments-27-4155034.0-0"""">support for WSL</a>.</p>""";;;
"Gitlab,Assess,Tools,TRUE,""<p>One of the patterns we've seen repeat itself in this publication is that static error- and style-checking tools emerge quickly after a new language gains popularity. These tools are generically known as linters — after the classic and beloved Unix utility <em>lint</em>, which statically analyzes C code. We like these tools because they catch errors early, before code even gets compiled. The latest instance of this pattern is <strong><a href=""""https://stoplight.io/open-source/spectral/"""">Spectral</a></strong>, a linter for YAML and JSON. Although Spectral is a generic tool for these formats, its main target is OpenAPI (the evolution of <a href=""""/radar/tools/swagger"""">Swagger</a>) and <a href=""""/radar/tools/asyncapi"""">AsyncAPI</a>. Spectral ships with a comprehensive set of out-of-the-box rules for these specs that can save developers headaches when designing and implementing APIs or event-driven collaboration. These rules check for proper API parameter specifications or the existence of a license statement in the spec, among other things. While this tool is a welcome addition to the API development workflow, it does raise the question of whether a non-executable specification should be so complex as to require an error-checking technique designed for programming languages. Perhaps developers should be writing code instead of specs?</p>""";;;
Oracle Cloud,Hold,Platforms,TRUE,;;;
IBM Cloud,Hold,Platforms,TRUE,;;;
APIGEE,Adopt,Platforms,TRUE,;;;
".NET,Adopt,languages-and-frameworks,TRUE,""<p>A long time ago we placed <a href=""""/radar/languages-and-frameworks/reactivex"""">ReactiveX</a> — a family of open-source frameworks for reactive programming — into the Adopt ring of the Radar. In 2017, we mentioned the addition of <a href=""""https://github.com/ReactiveX/RxSwift"""">RxSwift</a>, which brought reactive programming to iOS development using Swift. Since then, Apple has introduced its own take on reactive programming in the form of the <a href=""""https://developer.apple.com/documentation/combine""""><strong>Combine</strong></a> framework. Combine has become our default choice for apps that support iOS 13 as an acceptable deployment target. It's easier to learn than RxSwift and integrates really well with <a href=""""/radar/languages-and-frameworks/swiftui"""">SwiftUI</a>. If you're planning to convert an existing application from RxSwift to Combine or work with both in the same project, you might want to look at <a href=""""https://github.com/CombineCommunity/RxCombine"""">RxCombine</a>.</p>""";;;
"NetCore,Trial,languages-and-frameworks,FALSE,""<p>Our mobile teams now view <strong><a href=""""http://github.com/square/leakcanary"""">LeakCanary</a></strong> as a good default choice for Android development. It detects annoying memory leaks in Android apps, is extremely simple to hook up and provides notifications with a clear trace-back to the cause of the leak. LeakCanary can save you tedious hours troubleshooting out-of-memory errors on multiple devices, and we recommend you add it to your toolkit.</p>""";;;
"Go,Trial,languages-and-frameworks,TRUE,""<p>As we continue developing web applications in JavaScript, we continue enjoying the <a href=""""/radar/languages-and-frameworks/testing-library"""">Testing Library</a> approach of testing applications";" and carry on exploring and gaining experience with its packages — beyond that of <a href=""""/radar/languages-and-frameworks/react-testing-library"""">React Testing Library</a>. <strong><a href=""""https://testing-library.com/docs/angular-testing-library/intro/"""">Angular Testing Library</a></strong> brings all the benefits of its family when testing UI components in a user-centric way, pushing for more maintainable tests focused primarily on behavior rather than testing UI implementation details. Although it falls short in documentation, Angular Testing Library does provide <a href=""""https://github.com/testing-library/angular-testing-library/tree/master/apps/example-app/app/examples"""">good sample tests</a> that helped us in getting started faster for various cases. We've had great success with this testing library in our <a href=""""/radar/languages-and-frameworks/angular"""">Angular</a> projects and advise you to trial this solid testing approach.</p>""";;
"Quarkus,Trial,languages-and-frameworks,TRUE,""<p><strong><a href=""""https://github.com/awslabs/aws-data-wrangler"""">AWS Data Wrangler</a></strong> is an open-source library that extends the capabilities of <a href=""""https://github.com/pandas-dev/pandas"""">Pandas</a> to AWS by connecting data frames to AWS data-related services. In addition to Pandas, this library leverages <a href=""""https://github.com/apache/arrow"""">Apache Arrow</a> and <a href=""""https://github.com/boto/boto3"""">Boto3</a> to expose <a href=""""https://aws-data-wrangler.readthedocs.io/en/2.5.0/api.html"""">several APIs</a> to load, transform and save data from data lakes and data warehouses. An important limitation is that you can't do large distributed data pipelines with this library. However, you can leverage the native data services — like Athena, Redshift and Timestream — to do the heavy lifting and pull data in order to express complex transformations that are well suited for data frames. We've used AWS Data Wrangler in production and like that it lets you focus on writing transformations without spending too much time on the connectivity to AWS data services.</p>""";;;
"SpringBoot,Adopt,languages-and-frameworks,FALSE,""<p>Although JavaScript and its ecosystem is dominant in the web UI development space, new opportunities are opening up with the emergence of <a href=""""/radar/languages-and-frameworks/webassembly"""">WebAssembly</a>. <strong><a href=""""https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor"""">Blazor</a></strong> continues to demand our attention";" it's producing good results with our teams building interactive rich user interfaces using C# on top of WebAssembly. The fact that our teams can use C# on the frontend too allows them to share code and reuse existing libraries. That, along with the existing tooling for debugging and testing, such as <a href=""""/radar/languages-and-frameworks/bunit"""">bUnit</a>, make this open-source framework worth trying.</p>""";;
"Angular,Adopt,languages-and-frameworks,TRUE,""<p>We're seeing more teams adopting Python as the preferred language to build solutions, not just for data science but for back-end services too. In these scenarios, we're having good experiences with <strong><a href=""""https://fastapi.tiangolo.com/"""">FastAPI</a></strong> — a modern, fast (high-performance), web framework for building APIs with Python 3.6 or later. Additionally, this framework and its ecosystem include features such as API documentation using OpenAPI that allow our teams to focus on the business functionalities and quickly create REST APIs, which makes FastAPI a good alternative to existing solutions in this space.</p>""";;;
"Node JS,Adopt,languages-and-frameworks,FALSE,""<p>We've really enjoyed using <a href=""""/radar/languages-and-frameworks/typescript"""">TypeScript</a> for a while now and love the safety that the strong typing provides. However, getting data into the bounds of the type system — from, for example, a call to a back-end service — can lead to run-time errors. One library that helps solve this problem is <strong><a href=""""https://gcanti.github.io/io-ts/"""">io-ts</a></strong>. It bridges the gap between compile-time type-checking and run-time consumption of external data by providing encode and decode functions. It can also be used as a custom type guard. As we gain more experience with io-ts in our work, our initially positive impressions are confirmed, and we still like the elegance of its approach.</p>""";;;
"Kotlin,Assess,languages-and-frameworks,TRUE,""<p>The introduction of <a href=""""https://kotlinlang.org/docs/coroutines-overview.html"""">coroutines to Kotlin</a> opened the door for several innovations — <strong><a href=""""https://kotlinlang.org/docs/flow.html"""">Kotlin Flow</a></strong> is one of them, directly integrated into the coroutines library. It's an implementation of Reactive Streams on top of coroutines. Unlike <a href=""""https://github.com/ReactiveX/RxJava"""">RxJava</a>, flows are a native Kotlin API similar to the familiar sequence API with methods that include <code>map</code> and <code>filter</code>. Like sequences, flows are <em>cold</em>, meaning that the values of the sequence are only constructed when needed. All of this makes writing multithreaded code much simpler and easier to understand than other approaches. The <code>toList</code> method, predictably, converts a flow into a list which is a common pattern in tests.</p>""";;;
Phyton,Adopt,languages-and-frameworks,TRUE,;;;
Scala,Hold,languages-and-frameworks,TRUE,;;;
Ruby,Hold,languages-and-frameworks,TRUE,;;;
C / C++,Trial,languages-and-frameworks,TRUE,;;;
3Scale,Assess,Platforms,TRUE,;;;
DataPower,Hold,Platforms,TRUE,;;;
IBM WAS,Hold,Platforms,TRUE,;;;
Linux OS,Adopt,Platforms,TRUE,;;;
Windows OS,Trial,Platforms,TRUE,;;;
Canary Deployment,Trial,Techniques,TRUE,;;;